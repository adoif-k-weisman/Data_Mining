# 实验一：PCA

## 一、实验目的

掌握 PCA 算法的原理

## 二、实验要求

通过本实验应达到如下要求：

- 理解数据降维过程
- 熟练使用 Python 或其他工具实现 PCA 算法

## 三、实验器材

- 计算机一台
- Python 或其他编程工具

## 四、实验内容

主成分分析（PCA）是一种提取数据集最重要特征的统计程序。PCA 的一个关键点是降维。降维是减少给定数据集的维数的过程。PCA 使我们能够找到数据变化最大的方向。

简而言之，使用 PCA 可以让我们找到数据集中的一些最重要的组成部分，即在描述某件事时一些最重要的变量。

### 1.PCA的原理

​	PCA的本质就是找一些投影方向，使得数据在这些投影方向上的方差最大，而且这些投影方向是相互正交的。这其实就是找新的正交基的过程，计算原始数据在这些正交基上投影的方差，方差越大，就说明在对应正交基上包含了更多的信息量。如果特征值较小，则说明数据在这些特征向量上投影的信息量很小，我们就可以将小特征值对应方向的数据删除，从而达到了降维的目的。

### 2. PCA的主要流程

​	PCA 所要做的工作，简单点说，就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的，这样假设在 N 维空间中，我们可以找到 N 个这样的坐标轴，我们取前 r 个去近似这个空间，这样就从一个 N 维的空间压缩到 r 维的空间了，但是我们选择的 r 个坐标轴能够使得空间的压缩使得数据的损失最小。

​	所以进行PCA实验的关键点在于：如何找到新的投影方向使得原始数据的“信息量”损失最少

​	而协方差矩阵的特征向量，它总是指向数据方差最大的方向。那么我们只要对特征值较大的特征向量的方向做投影即可达到PCA降维的要求的同时，最大程度地减少原始数据的“信息量”损失。

### 3.PCA降维过程

#### (1).**对原始数据零均值化（中心化）**

​	中心化即是指变量减去它的均值，使均值为0。其实就是一个平移的过程，平移后使得所有数据的中心是（0,0）。只有中心化数据之后，计算得到的方向才能比较好的概括原来的数据。

#### (2).**求协方差矩阵**

我们对于一组数据，如果它在某一坐标轴上的方差越大，说明坐标点越分散，该属性能够比较好的反映源数据。所以在进行降维的时候，主要目的是找到一个超平面，它能使得数据点的分布方差呈最大，这样数据表现在新的坐标轴上时候已经足够分散了。

 PCA算法的优化目标就是：降维后同一维度的方差最大；不同维度之间的相关性为0。

协方差就是一种用来度量两个随机变量关系的统计量。同一元素的协方差就表示该元素的方差，不同元素之间的协方差就表示他们之间的相关性。 而当实验数据涉及到多个维度时，就需要用到协方差矩阵。

#### (3).**对协方差矩阵求特征向量和特征值**

对于协方差矩阵A，其特征值 ![\lambda](https://latex.codecogs.com/gif.latex?%5Clambda)计算方法为：![\left | A-\lambda E \right |=0](https://latex.codecogs.com/gif.latex?%5Cleft%20%7C%20A-%5Clambda%20E%20%5Cright%20%7C%3D0)

将求出的特征值按照从大到小的顺序排序，选择其中最大的K个，然后将其对应的K个特征向量分别作为列向量组成特征向量矩阵W，将数据集投影到选取的特征向量上，这样就得到了我们需要的已经降维的数据集。

### 4.实验选用数据集

**数据集选取为：** 本次实验使用sklearn库中的iris数据集，其中每个样本有4个特征参数，分别为花萼长度，花萼宽度，花瓣长度，花瓣宽度4个属性。

**加载数据集代码：**

```python
def loadData():
    iris = datasets.load_iris()
    # 只取前五行数据进行计算
    X = iris.data
    y = iris.target
    return X
```

### 5.方法一

（a）**数据标准化（去均值）代码：**

 ```python
    mean = np.mean(data, axis=0)
    # 去中心化
    # 从每行数据（每次观测中）减去均值：
    data_scaled = data - mean
 ```

​	**标准化后的数据（前五行）为：**

 ```python
[[-0.74333333  0.44266667 -2.358      -0.99933333]
 [-0.94333333 -0.05733333 -2.358      -0.99933333]
 [-1.14333333  0.14266667 -2.458      -0.99933333]
 [-1.24333333  0.04266667 -2.258      -0.99933333]
 [-0.84333333  0.54266667 -2.358      -0.99933333]]
 ```



（b）**求协方差矩阵代码：**

 ```python
    len_matrix = len(data_scaled)
    # 矩阵转置
    data_scaled = Matrix(data_scaled).T
    # 求协方差矩阵
    cov = data_scaled * Matrix(data_scaled).T
    cov = cov / (len_matrix-1)
 ```

**协方差矩阵结果为：**

 ```python
[[0.685693512304250 -0.0424340044742729 1.27431543624161   0.516270693512304]
 [-0.0424340044742729 0.189979418344519 -0.329656375838926 -0.121639373601790]
 [1.27431543624161 -0.329656375838926   3.11627785234899    1.29560939597315]
 [0.516270693512304 -0.121639373601790  1.29560939597315   0.581006263982103]]
 ```

（c）**求特征值和特征向量代码：**

 ```python
    eigen_vector = [list(line[2][0]) for line in cov.eigenvects()]
    eigen_value = list(cov.eigenvals().keys())
 ```

**特征值及特征向量结果为：**

 ```python
# 特征值
[4.22824170603486 0.242670747928633 0.0782095000429191 0.0238350929734502]
# 特征向量
[[0.361386591785368 -0.0845225140645688 0.856670605949835  0.358289197151551]
 [-0.656588771286842 -0.730161434785026 0.173372662795858  0.0754810199174624]
 [-0.582029851306065 0.597910830100086 0.0762360758209631  0.545831432020076]
 [-0.315487192903975 0.319723103666131 0.479838986994634  -0.753657425264046]]
 ```

（d）将数据降到k维（k的值可以依据原数据集选取，如果使用iris数据集，k可以取值为2），按特征值大小排序，选取前k个特征值对应的特征向量，计算**降维后的数据（前五行）为：**

```python
# 降到二维的数据
[[-2.68412562596954 -0.319397246585101]
 [-2.71414168729433 0.177001225064780]
 [-2.88899056905930 0.144949426085558]
 [-2.74534285564141 0.318298979251917]
 [-2.72871653655453 -0.326754512934919]]
```



### 6.方法二

用sklearn进行PCA，from sklearn.decomposition import PCA

**降至二维后的数据（前五行）为：**

```python
[[-2.68412563  0.31939725 -0.02791483]
 [-2.71414169 -0.17700123 -0.21046427]
 [-2.88899057 -0.14494943  0.01790026]
 [-2.74534286 -0.31829898  0.03155937]
 [-2.72871654  0.32675451  0.09007924]]
```



## **5.**  实验心得

​	通过本次实验，我熟练了对于Python的numpy的各种运用，掌握了利用Python作为工具求解矩阵的协方差矩阵以及协方差矩阵的特征值和特征向量，加强了我对于Python各种数据格式之间的转换操作，无论是数组还是矩阵，抑或是对于数组切片取得数据的方式。同时，我本次实验最大的收获就是学习到了PCA算法的各种原理，能够自己实现PCA算法，掌握了一种数据挖掘中数据处理的方式，对于数据挖掘也有了初步的认知。



>数学知识补充：
>
>1. 协方差矩阵：协方差矩阵的作用：协方差是用来衡量两个变量之间“协同变异大小的总体参数，即二个变量相互影响的大小的参数，协方差的绝对值越大，则两个变量相互影响越大。”
>2. 特征向量：对协方差矩阵的特征向量最直观的解释之一是：它总是指向数据方差最大的方向。
>更准确地说，第一特征向量是数据方差最大的方向，第二特征向量是与第一特征向量垂直方向上数据方差最大的方向，第三特征向量是与第一和第二特征向量垂直的方向上数据方差最大的方向，以此类推。
>3. 协方差矩阵表示了样本集在原n维空间中各个方向上的能量分布，通过对协方差矩阵求特征向量，实际上找到的是在原n维空间中的一些特定的方向，样本集的能量集中分布在这些方向上，而特征值的大小就反映了样本集在该方向上的能量大小。PCA正式基于这一点，删掉对应特征值小的方向，只保留主要的方向，达到降维的目的。
>4. PCA算法的实质就是在能尽可能好的代表原特征的情况下，将原特征进行线性变换、映射至低纬度空间中。
>5. 在第一步减均值之后，其实应该还有一步对特征做方差归一化。比如一个特征是汽车速度（0到100），一个是汽车的座位数（2到6），显然第二个的方差比第一个小。因此，如果样本特征中存在这种情况，那么在第一步之后，求每个特征的标准差，然后对每个样例在该特征下的数据除以标准差。